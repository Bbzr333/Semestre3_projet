{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet HAI923 - Mod√®le CLIP Image-Texte\n",
    "\n",
    "**Nom:** [√Ä COMPL√âTER]  \n",
    "**Pr√©nom:** [√Ä COMPL√âTER]  \n",
    "**N¬∞ Carte √âtudiant:** [√Ä COMPL√âTER]  \n",
    "**Num√©ro de Groupe:** [√Ä COMPL√âTER]\n",
    "\n",
    "---\n",
    "\n",
    "## Description\n",
    "\n",
    "R√©alisation d'un mod√®le CLIP (Contrastive Language-Image Pre-training) pour associer des images et des textes.\n",
    "\n",
    "**Dataset:** Flickr - 4 classes (\"bike\", \"ball\", \"water\", \"dog\") - 600 paires image-texte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Importations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standards\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "# Transformers (pour SmallBERT)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# PIL pour les images\n",
    "from PIL import Image\n",
    "\n",
    "# Utilitaires projet\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from config import *\n",
    "from utils import *\n",
    "\n",
    "# Configuration\n",
    "set_seed(RANDOM_SEED)\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Chargement et Exploration des Donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Charger les donn√©es Flickr depuis ProjetClip.ipynb\n",
    "# - Images: 4 classes x 150 images\n",
    "# - Textes: captions associ√©es\n",
    "\n",
    "# Exemple de structure de donn√©es attendue:\n",
    "# data = {\n",
    "#     'image_path': [...],\n",
    "#     'caption': [...],\n",
    "#     'label': [...]  # 0: bike, 1: ball, 2: water, 3: dog\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ √âTAPE 1: Classifieur CNN pour Images (4 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset et DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Dataset pour les images Flickr\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Impl√©menter le chargement d'image\n",
    "        pass\n",
    "\n",
    "# Transformations pour les images\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# TODO: Cr√©er les DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Architecture CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"CNN simple pour classification d'images\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=4):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        # TODO: D√©finir l'architecture\n",
    "        # Suggestion: Conv2D -> ReLU -> MaxPool -> ... -> Flatten -> Dense\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Impl√©menter le forward pass\n",
    "        pass\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "cnn_model = SimpleCNN(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "print(f\"Nombre de param√®tres: {count_parameters(cnn_model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Entra√Ænement CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entra√Æner le CNN\n",
    "# ATTENTION: Ne pas perdre de temps √† optimiser, juste un mod√®le fonctionnel\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=CNN_CONFIG['learning_rate'])\n",
    "\n",
    "# Boucle d'entra√Ænement\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 √âvaluation CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: √âvaluer le CNN sur le test set\n",
    "# Afficher: accuracy, confusion matrix, exemples de pr√©dictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ √âTAPE 2: Classifieur SmallBERT pour Textes (4 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dataset et DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger SmallBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(SMALLBERT_CONFIG['model_name'])\n",
    "smallbert_base = AutoModel.from_pretrained(SMALLBERT_CONFIG['model_name'])\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"Dataset pour les textes Flickr\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Tokenizer le texte\n",
    "        pass\n",
    "\n",
    "# TODO: Cr√©er les DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Architecture SmallBERT Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallBERTClassifier(nn.Module):\n",
    "    \"\"\"Classifieur bas√© sur SmallBERT\"\"\"\n",
    "    \n",
    "    def __init__(self, smallbert_model, num_classes=4, hidden_size=512):\n",
    "        super(SmallBERTClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = smallbert_model\n",
    "        \n",
    "        # ATTENTION: SmallBERT n'a PAS de token <CLS>\n",
    "        # TODO: Comment r√©sumer la phrase?\n",
    "        # Options:\n",
    "        # 1. Mean pooling sur tous les tokens\n",
    "        # 2. Max pooling\n",
    "        # 3. Utiliser le dernier hidden state\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # TODO: Impl√©menter le forward pass\n",
    "        pass\n",
    "\n",
    "# Cr√©er le mod√®le\n",
    "smallbert_classifier = SmallBERTClassifier(smallbert_base, NUM_CLASSES).to(DEVICE)\n",
    "print(f\"Nombre de param√®tres: {count_parameters(smallbert_classifier):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Entra√Ænement SmallBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entra√Æner le mod√®le SmallBERT\n",
    "# ATTENTION: Ne pas perdre de temps √† optimiser\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(smallbert_classifier.parameters(), \n",
    "                        lr=SMALLBERT_CONFIG['learning_rate'])\n",
    "\n",
    "# Boucle d'entra√Ænement\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 √âvaluation SmallBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: √âvaluer le classifieur de textes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ √âTAPE 3: Mod√®le CLIP (C≈íUR DU PROJET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Encodeur Image (CNN sans classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Encodeur image pour CLIP\"\"\"\n",
    "    \n",
    "    def __init__(self, cnn_model, embedding_dim=512):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        \n",
    "        # TODO: Retirer les couches de classification du CNN\n",
    "        # Garder jusqu'au flatten inclus\n",
    "        \n",
    "        # Projection vers l'espace d'embeddings\n",
    "        # SANS fonction d'activation\n",
    "        self.projection = nn.Linear(???, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: Impl√©menter le forward\n",
    "        # N'OUBLIEZ PAS: normaliser la sortie\n",
    "        pass\n",
    "\n",
    "# Cr√©er l'encodeur image\n",
    "image_encoder = ImageEncoder(cnn_model, CLIP_CONFIG['embedding_dim']).to(DEVICE)\n",
    "print(f\"Image Encoder - Param√®tres: {count_parameters(image_encoder):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Encodeur Texte (SmallBERT sans classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"Encodeur texte pour CLIP\"\"\"\n",
    "    \n",
    "    def __init__(self, bert_model, embedding_dim=512, hidden_size=512):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        \n",
    "        self.bert = bert_model\n",
    "        \n",
    "        # Projection vers l'espace d'embeddings\n",
    "        # SANS fonction d'activation\n",
    "        self.projection = nn.Linear(hidden_size, embedding_dim)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # TODO: Impl√©menter le forward\n",
    "        # R√©sumer la phrase (mean pooling, etc.)\n",
    "        # N'OUBLIEZ PAS: normaliser la sortie\n",
    "        pass\n",
    "\n",
    "# Cr√©er l'encodeur texte\n",
    "text_encoder = TextEncoder(smallbert_base, CLIP_CONFIG['embedding_dim']).to(DEVICE)\n",
    "print(f\"Text Encoder - Param√®tres: {count_parameters(text_encoder):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Mod√®le CLIP Complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModel(nn.Module):\n",
    "    \"\"\"Mod√®le CLIP combinant image et texte\"\"\"\n",
    "    \n",
    "    def __init__(self, image_encoder, text_encoder):\n",
    "        super(CLIPModel, self).__init__()\n",
    "        \n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "    \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Encoder les images et les textes\n",
    "        image_embeddings = self.image_encoder(images)\n",
    "        text_embeddings = self.text_encoder(input_ids, attention_mask)\n",
    "        \n",
    "        return image_embeddings, text_embeddings\n",
    "\n",
    "# Cr√©er le mod√®le CLIP\n",
    "clip_model = CLIPModel(image_encoder, text_encoder).to(DEVICE)\n",
    "print(f\"CLIP Model - Total Param√®tres: {count_parameters(clip_model):,}\")\n",
    "\n",
    "# V√©rifier que les dimensions correspondent\n",
    "print(f\"\\nDimension embeddings: {CLIP_CONFIG['embedding_dim']}\")\n",
    "print(\"‚úÖ Les dimensions doivent √™tre identiques pour image et texte!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Loss Contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La loss contrastive est d√©j√† d√©finie dans utils.py\n",
    "contrastive_loss = ContrastiveLoss(temperature=CLIP_CONFIG['temperature'])\n",
    "\n",
    "print(\"Loss Contrastive initialis√©e\")\n",
    "print(f\"Temp√©rature: {CLIP_CONFIG['temperature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Dataset CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    \"\"\"Dataset pour CLIP (image + texte)\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, captions, tokenizer, transform=None, max_length=128):\n",
    "        self.image_paths = image_paths\n",
    "        self.captions = captions\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: Charger image + tokenizer texte\n",
    "        pass\n",
    "\n",
    "# TODO: Cr√©er les DataLoaders CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Entra√Ænement CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entra√Æner le mod√®le CLIP\n",
    "\n",
    "optimizer = optim.Adam(clip_model.parameters(), lr=CLIP_CONFIG['learning_rate'])\n",
    "\n",
    "# Boucle d'entra√Ænement\n",
    "for epoch in range(CLIP_CONFIG['num_epochs']):\n",
    "    # TODO: Training loop\n",
    "    pass\n",
    "\n",
    "# CRITIQUE: V√©rifier la sauvegarde/rechargement\n",
    "save_model(clip_model, PATHS['clip_model'])\n",
    "clip_model, _ = load_model(clip_model, PATHS['clip_model'])\n",
    "print(\"‚úÖ Sauvegarde/rechargement test√© avec succ√®s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Inf√©rence CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 Texte ‚Üí Images (Top-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_images(text_query, clip_model, image_dataset, tokenizer, top_k=5):\n",
    "    \"\"\"\n",
    "    Trouve les top-k images correspondant au texte\n",
    "    \n",
    "    Returns:\n",
    "        List[(image, score)]: Top-k images avec leurs scores\n",
    "    \"\"\"\n",
    "    # TODO: Impl√©menter l'inf√©rence texte ‚Üí images\n",
    "    # 1. Encoder le texte\n",
    "    # 2. Encoder toutes les images\n",
    "    # 3. Calculer similarit√©s\n",
    "    # 4. Retourner top-k avec scores\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "query = \"A big dog in the woods\"\n",
    "results = text_to_images(query, clip_model, test_images, tokenizer)\n",
    "display_top_k_results(query, results, query_type=\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2 Image ‚Üí Textes (Top-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_texts(image, clip_model, text_dataset, top_k=5):\n",
    "    \"\"\"\n",
    "    Trouve les top-k textes correspondant √† l'image\n",
    "    \n",
    "    Returns:\n",
    "        List[(text, score)]: Top-k textes avec leurs scores\n",
    "    \"\"\"\n",
    "    # TODO: Impl√©menter l'inf√©rence image ‚Üí textes\n",
    "    pass\n",
    "\n",
    "# Test\n",
    "test_image = # TODO: Charger une image de test\n",
    "results = image_to_texts(test_image, clip_model, test_captions)\n",
    "display_top_k_results(test_image, results, query_type=\"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üîß TRAVAIL FACULTATIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option A: Remplacer SmallBERT par DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (facultatif): Utiliser DistilBERT\n",
    "# from transformers import DistilBertTokenizerFast, DistilBertModel\n",
    "# ATTENTION: V√©rifier l'alignement des dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option B: Enrichir les textes courts via LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (facultatif): Enrichir les captions courtes\n",
    "# Conserver la m√™me s√©mantique\n",
    "# Les textes sont dans caption.csv et dans le r√©pertoire caption/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä R√©sultats et Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pr√©senter les r√©sultats\n",
    "# - Courbes d'entra√Ænement\n",
    "# - Exemples de requ√™tes texte ‚Üí images\n",
    "# - Exemples de requ√™tes image ‚Üí textes\n",
    "# - Analyse qualitative des r√©sultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ CHECKLIST FINALE\n",
    "\n",
    "Avant de rendre le projet, v√©rifier:\n",
    "\n",
    "- [ ] Nom, pr√©nom, n¬∞ carte √©tudiant de tous les membres\n",
    "- [ ] CNN fonctionnel (√©tape 1)\n",
    "- [ ] SmallBERT fonctionnel (√©tape 2)\n",
    "- [ ] Mod√®le CLIP complet (√©tape 3)\n",
    "- [ ] Dimensions embeddings identiques (image et texte)\n",
    "- [ ] Normalisation des embeddings activ√©e\n",
    "- [ ] Loss contrastive int√©gr√©e\n",
    "- [ ] Sauvegarde/rechargement test√©\n",
    "- [ ] Inf√©rence texte ‚Üí images (top-5 + scores)\n",
    "- [ ] Inf√©rence image ‚Üí textes (top-5 + scores)\n",
    "- [ ] Fichiers nomm√©s: `[GROUPE]_*.ipynb` et `[GROUPE]_*.pdf`\n",
    "- [ ] Rapport LaTeX ‚â§ 8 pages (+ ‚â§ 2 pages annexes)\n",
    "- [ ] Archive `[GROUPE].zip` avec tous les fichiers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
