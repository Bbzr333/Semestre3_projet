"""
Version simplifi√©e - Comparaison avec ChatGPT
Utilise requests au lieu de la biblioth√®que OpenAI
"""

import pandas as pd
import numpy as np
import sys
import json
import time
import requests
from pathlib import Path
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm
import os

sys.path.append('src')

# Configuration
API_URL = "https://api.openai.com/v1/chat/completions"

RELATIONS_DEFINITIONS = {
    'r_has_causatif': "Relation de cause (A cause B ou B cause A)",
    'r_has_property-1': "A poss√®de la propri√©t√© B",
    'r_objet>matiere': "A est fait de la mati√®re B",
    'r_lieu>origine': "A provient du lieu B",
    'r_topic': "A a pour sujet/th√®me B",
    'r_depic': "A repr√©sente/d√©peint B",
    'r_holo': "A est une partie de B (relation partie-tout)",
    'r_lieu': "A est situ√© dans/√† B",
    'r_processus_agent': "A est l'agent qui effectue le processus B",
    'r_processus_patient': "A subit le processus B",
    'r_processus>instr-1': "A est l'instrument utilis√© pour B",
    'r_own-1': "A appartient √† B (possession)",
    'r_quantificateur': "A est une quantit√© de B",
    'r_social_tie': "Lien social entre A et B",
    'r_product_of': "A est le produit/r√©sultat de B"
}

def create_few_shot_prompt(train_df, n_examples_per_class=2):
    """Cr√©e un prompt few-shot"""
    prompt = """Tu es un expert en analyse linguistique fran√ßaise. Ta t√¢che est d'identifier la relation s√©mantique dans les constructions g√©nitives "A de B".

RELATIONS POSSIBLES:
"""
    for rel, definition in RELATIONS_DEFINITIONS.items():
        prompt += f"\n‚Ä¢ {rel}: {definition}"
    
    prompt += "\n\nEXEMPLES:\n"
    
    for relation in sorted(train_df['type_jdm'].unique()):
        examples = train_df[train_df['type_jdm'] == relation].sample(
            min(n_examples_per_class, len(train_df[train_df['type_jdm'] == relation]))
        )
        for _, row in examples.iterrows():
            prompt += f'\nPhrase: "{row["phrase_originale"]}"\nRelation: {relation}\n'
    
    prompt += """\n\nINSTRUCTIONS:
1. Analyse la construction "A de B"
2. Identifie la relation s√©mantique
3. R√©ponds UNIQUEMENT avec le nom exact de la relation
4. Ne donne aucune explication

Format: juste le nom de la relation.
"""
    return prompt

def query_chatgpt_api(api_key, phrase, system_prompt, model="gpt-3.5-turbo", max_retries=3):
    """Interroge ChatGPT via requests"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f'Phrase: "{phrase}"\nRelation:'}
        ],
        "temperature": 0,
        "max_tokens": 50
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(API_URL, headers=headers, json=data, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            prediction = result['choices'][0]['message']['content'].strip()
            prediction = prediction.replace('.', '').replace(',', '').strip()
            
            # V√©rifier si c'est une relation valide
            if prediction in RELATIONS_DEFINITIONS.keys():
                return prediction
            else:
                for rel in RELATIONS_DEFINITIONS.keys():
                    if rel in prediction:
                        return rel
                return prediction
        
        except requests.exceptions.RequestException as e:
            if attempt < max_retries - 1:
                print(f"  ‚ö†Ô∏è  Erreur (tentative {attempt + 1}/{max_retries}): {e}")
                time.sleep(2 ** attempt)
            else:
                print(f"  ‚ùå √âchec: {e}")
                return "ERROR"
    
    return "ERROR"

def main():
    print("=" * 70)
    print("ü§ñ COMPARAISON AVEC CHATGPT (Version Simplifi√©e)")
    print("=" * 70)
    
    # Demander la cl√© API
    
    api_key = None

    # 1. Essayer de charger depuis config.json
    try:
        with open('data/apiKey/config.json', 'r') as f:
            config = json.load(f)
            api_key = config.get('openai_api_key')
            if api_key:
                print(f"‚úÖ Cl√© API charg√©e depuis config.json")
    except FileNotFoundError:
        print("‚ö†Ô∏è  Fichier config.json non trouv√©")
    except json.JSONDecodeError:
        print("‚ö†Ô∏è  Erreur de lecture du fichier config.json")
    
    if not api_key:
        print("\nüîë Cl√© API OpenAI non trouv√©e dans les variables d'environnement")
        api_key = input("Entrez votre cl√© API OpenAI (sk-...): ").strip()
        
        if not api_key.startswith('sk-'):
            print("‚ùå Cl√© API invalide (doit commencer par 'sk-')")
            return
    
    print(f"‚úÖ Cl√© API configur√©e")
    
    # Charger les donn√©es
    print(f"\nüìÇ Chargement des donn√©es...")
    train = pd.read_csv('data/processed/train.csv')
    test = pd.read_csv('data/processed/test.csv')
    
    print(f"‚úì Train: {len(train)} exemples")
    print(f"‚úì Test: {len(test)} exemples")
    
    # Configuration
    MODEL = 'gpt-3.5-turbo'  # Moins cher pour commencer
    N_SAMPLES = 50  # Petit √©chantillon
    
    print(f"\n‚öôÔ∏è  Configuration:")
    print(f"  ‚Ä¢ Mod√®le: {MODEL}")
    print(f"  ‚Ä¢ √âchantillon: {N_SAMPLES} exemples")
    print(f"  ‚Ä¢ Co√ªt estim√©: ~${(N_SAMPLES * 0.002):.2f}")
    
    response = input("\n‚ñ∂Ô∏è  Continuer? (y/n): ")
    if response.lower() != 'y':
        print("‚ùå Annul√©")
        return
    
    # √âchantillonner
    test_sample = test.sample(N_SAMPLES, random_state=42)
    
    # Cr√©er le prompt
    print(f"\nüîß Cr√©ation du prompt few-shot...")
    system_prompt = create_few_shot_prompt(train, n_examples_per_class=2)
    
    # √âvaluer
    print(f"\nüöÄ Interrogation de {MODEL}...")
    predictions = []
    y_true = []
    errors = []
    
    start_time = time.time()
    
    for idx, row in tqdm(test_sample.iterrows(), total=len(test_sample)):
        phrase = row['phrase_originale']
        true_label = row['type_jdm']
        
        prediction = query_chatgpt_api(api_key, phrase, system_prompt, model=MODEL)
        
        predictions.append(prediction)
        y_true.append(true_label)
        
        if prediction != true_label:
            errors.append({
                'phrase': phrase,
                'true': true_label,
                'pred': prediction
            })
        
        time.sleep(0.2)  # Rate limiting
    
    elapsed_time = time.time() - start_time
    
    # R√©sultats
    print(f"\n{'='*70}")
    print(f"üìä R√âSULTATS")
    print(f"{'='*70}")
    
    valid_predictions = [p for p in predictions if p != "ERROR"]
    valid_true = [y_true[i] for i, p in enumerate(predictions) if p != "ERROR"]
    
    accuracy = accuracy_score(valid_true, valid_predictions)
    print(f"\n‚úÖ Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)")
    print(f"‚è±Ô∏è  Temps: {elapsed_time:.1f}s ({elapsed_time/N_SAMPLES:.2f}s/exemple)")
    print(f"‚ùå Erreurs: {len(errors)}/{N_SAMPLES}")
    
    if errors:
        print(f"\nüîç Exemples d'erreurs:")
        for i, err in enumerate(errors[:5]):
            print(f"  {i+1}. \"{err['phrase']}\"")
            print(f"     Vrai: {err['true']} | Pr√©dit: {err['pred']}")
    
    # Comparaison avec baseline
    baseline_results = pd.read_csv('results/test_results.csv', index_col=0)
    
    print(f"\n{'='*70}")
    print(f"üèÜ COMPARAISON AVEC BASELINE")
    print(f"{'='*70}")
    print(f"\nMod√®le                Accuracy    Temps/exemple")
    print(f"-" * 50)
    print(f"Random Forest         1.000       0.001s")
    print(f"{MODEL:20s}  {accuracy:.3f}       {elapsed_time/N_SAMPLES:.3f}s")
    
    print(f"\nüí° Conclusion:")
    if accuracy > 0.95:
        print(f"‚úÖ ChatGPT excellent ({accuracy:.1%}) mais {elapsed_time/N_SAMPLES/0.001:.0f}x plus lent")
    elif accuracy > 0.85:
        print(f"‚úÖ ChatGPT bon ({accuracy:.1%}) mais moins performant que RF (100%)")
    else:
        print(f"‚ö†Ô∏è  ChatGPT sous-performe ({accuracy:.1%}) vs RF (100%)")
    
    print(f"\n{'='*70}")
    print(f"‚úÖ √âVALUATION TERMIN√âE")
    print(f"{'='*70}")

if __name__ == '__main__':
    main()