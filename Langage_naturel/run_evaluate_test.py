"""
Ã‰valuation des modÃ¨les sur le test set
Analyse dÃ©taillÃ©e des performances et matrices de confusion
"""

import pandas as pd
import numpy as np
import sys
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

sys.path.append('src')

from models.baseline_models import BaselineClassifier
from evaluation.evaluator import ModelEvaluator

def plot_confusion_matrix(cm, labels, model_name, save_path):
    """Affiche et sauvegarde la matrice de confusion"""
    plt.figure(figsize=(14, 12))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=labels, yticklabels=labels,
                cbar_kws={'label': 'Nombre de prÃ©dictions'})
    plt.title(f'Matrice de Confusion - {model_name}', fontsize=16, fontweight='bold')
    plt.ylabel('Vraie Classe', fontsize=12)
    plt.xlabel('Classe PrÃ©dite', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.close()
    print(f"  ğŸ’¾ Matrice sauvegardÃ©e: {save_path}")

def analyze_errors(y_true, y_pred, X_test, test_df, model_name):
    """Analyse les erreurs de prÃ©diction"""
    errors = y_true != y_pred
    n_errors = errors.sum()
    
    if n_errors == 0:
        print(f"  ğŸ¯ Aucune erreur ! Performance parfaite sur le test set.")
        return
    
    print(f"  âŒ {n_errors} erreurs sur {len(y_true)} ({n_errors/len(y_true)*100:.1f}%)")
    
    # Analyser les confusions les plus frÃ©quentes
    error_df = pd.DataFrame({
        'true': y_true[errors],
        'pred': y_pred[errors],
        'phrase': test_df.loc[errors, 'phrase_originale'].values if 'phrase_originale' in test_df.columns else [''] * n_errors
    })
    
    confusion_pairs = error_df.groupby(['true', 'pred']).size().sort_values(ascending=False).head(5)
    
    if len(confusion_pairs) > 0:
        print(f"\n  ğŸ” Top 5 confusions:")
        for (true_label, pred_label), count in confusion_pairs.items():
            print(f"    â€¢ {true_label} â†’ {pred_label}: {count} fois")
            # Afficher un exemple
            example = error_df[(error_df['true'] == true_label) & (error_df['pred'] == pred_label)].iloc[0]
            if example['phrase']:
                print(f"      Exemple: \"{example['phrase']}\"")
    
    return error_df

def main():
    print("=" * 70)
    print("ğŸ§ª Ã‰VALUATION SUR LE TEST SET")
    print("=" * 70)
    
    # Chargement du test set
    print("\nğŸ“‚ Chargement des donnÃ©es...")
    test = pd.read_csv('data/processed/test.csv')
    print(f"âœ“ Test: {len(test)} exemples")
    
    # PrÃ©paration des features (mÃªme traitement que training)
    excluded_cols = ['phrase_originale', 'nom1_lemme', 'nom2_lemme', 'type_jdm', 'definitude',
                     'nom1', 'nom2', 'determinant']
    numeric_cols = test.select_dtypes(include=['int64', 'float64', 'int32', 'float32', 'bool']).columns.tolist()
    feature_cols = [col for col in numeric_cols if col not in excluded_cols]
    
    X_test = test[feature_cols]
    y_test = test['type_jdm']
    
    # Nettoyage (mÃªme que training)
    constant_cols = X_test.columns[X_test.std() == 0].tolist()
    if constant_cols:
        X_test = X_test.drop(columns=constant_cols)
    X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)
    
    print(f"âœ“ Features: {X_test.shape[1]}")
    print(f"âœ“ Classes: {y_test.nunique()}")
    
    # CrÃ©er les dossiers de sortie
    results_dir = Path('results')
    plots_dir = results_dir / 'plots'
    plots_dir.mkdir(parents=True, exist_ok=True)
    
    # Liste des modÃ¨les Ã  Ã©valuer
    models_dir = Path('models/baseline')
    model_files = list(models_dir.glob('*.joblib'))
    
    if not model_files:
        print("\nâŒ Aucun modÃ¨le trouvÃ© dans models/baseline/")
        return
    
    print(f"\nâœ“ {len(model_files)} modÃ¨les trouvÃ©s")
    
    # Ã‰valuation de chaque modÃ¨le
    all_results = {}
    
    for model_path in sorted(model_files):
        model_name = model_path.stem
        print(f"\n{'='*70}")
        print(f"ğŸ“Š Ã‰valuation: {model_name}")
        print(f"{'='*70}")
        
        try:
            # Charger le modÃ¨le
            classifier = BaselineClassifier.load(model_path)
            
            # PrÃ©dictions
            y_pred = classifier.predict(X_test)
            
            # MÃ©triques globales
            evaluator = ModelEvaluator()
            metrics = evaluator.evaluate(classifier, X_test, y_test)
            
            print(f"\nğŸ“ˆ MÃ©triques Globales:")
            print(f"  â€¢ Accuracy:  {metrics['accuracy']:.3f}")
            print(f"  â€¢ Precision: {metrics['precision_macro']:.3f}")
            print(f"  â€¢ Recall:    {metrics['recall_macro']:.3f}")
            print(f"  â€¢ F1-Score:  {metrics['f1_macro']:.3f}")
            
            # Matrice de confusion
            cm = metrics['confusion_matrix']
            labels = classifier.label_encoder.classes_
            
            plot_path = plots_dir / f'confusion_matrix_{model_name}.png'
            plot_confusion_matrix(cm, labels, model_name, plot_path)
            
            # Analyse des erreurs
            print(f"\nğŸ” Analyse des Erreurs:")
            error_df = analyze_errors(y_test, y_pred, X_test, test, model_name)
            
            # Sauvegarder les erreurs
            if error_df is not None and len(error_df) > 0:
                error_path = results_dir / f'errors_{model_name}.csv'
                error_df.to_csv(error_path, index=False)
                print(f"  ğŸ’¾ Erreurs sauvegardÃ©es: {error_path}")
            
            # Stocker les rÃ©sultats
            all_results[model_name] = {
                'accuracy': metrics['accuracy'],
                'precision': metrics['precision_macro'],
                'recall': metrics['recall_macro'],
                'f1_score': metrics['f1_macro'],
                'n_errors': (y_test != y_pred).sum()
            }
            
            # Rapport dÃ©taillÃ© par classe
            print(f"\nğŸ“‹ Rapport par Classe:")
            print(metrics['classification_report'])
            
        except Exception as e:
            print(f"  âŒ Erreur: {e}")
            all_results[model_name] = {
                'accuracy': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0,
                'n_errors': len(y_test),
                'error': str(e)
            }
    
    # Comparaison finale
    print(f"\n{'='*70}")
    print("ğŸ† COMPARAISON FINALE SUR TEST SET")
    print(f"{'='*70}")
    
    df_results = pd.DataFrame(all_results).T
    df_results = df_results.sort_values('accuracy', ascending=False)
    
    print("\nğŸ“Š Classement des ModÃ¨les:")
    print(df_results[['accuracy', 'f1_score', 'precision', 'recall', 'n_errors']].to_string())
    
    # Sauvegarder les rÃ©sultats
    results_path = results_dir / 'test_results.csv'
    df_results.to_csv(results_path)
    print(f"\nğŸ’¾ RÃ©sultats sauvegardÃ©s: {results_path}")
    
    # Meilleur modÃ¨le
    best_model = df_results.index[0]
    best_acc = df_results.iloc[0]['accuracy']
    best_errors = int(df_results.iloc[0]['n_errors'])
    
    print(f"\n{'='*70}")
    print(f"ğŸ¥‡ MEILLEUR MODÃˆLE: {best_model}")
    print(f"{'='*70}")
    print(f"  â€¢ Accuracy: {best_acc:.3f}")
    print(f"  â€¢ Erreurs:  {best_errors}/{len(y_test)}")
    print(f"  â€¢ Taux d'erreur: {best_errors/len(y_test)*100:.1f}%")
    
    # VÃ©rification de l'overfitting
    val_results = pd.read_csv('results/baseline_comparison.csv', index_col=0)
    
    print(f"\nğŸ” Analyse Overfitting:")
    print(f"{'ModÃ¨le':<25} {'Val Acc':<10} {'Test Acc':<10} {'Diff':<10} {'Status'}")
    print("-" * 70)
    
    for model in df_results.index:
        if model in val_results.index:
            val_acc = val_results.loc[model, 'accuracy']
            test_acc = df_results.loc[model, 'accuracy']
            diff = val_acc - test_acc
            
            if diff > 0.05:
                status = "âš ï¸ Overfitting"
            elif diff < -0.05:
                status = "ğŸ¯ Bonne gÃ©nÃ©ralisation"
            else:
                status = "âœ… Stable"
            
            print(f"{model:<25} {val_acc:<10.3f} {test_acc:<10.3f} {diff:+.3f}     {status}")
    
    print(f"\n{'='*70}")
    print("âœ… Ã‰VALUATION TERMINÃ‰E")
    print(f"{'='*70}")
    print(f"\nğŸ“ RÃ©sultats disponibles dans:")
    print(f"  â€¢ {results_path}")
    print(f"  â€¢ {plots_dir}/")

if __name__ == '__main__':
    main()